{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import cgitb\n",
    "import datetime\n",
    "import dateutil\n",
    "#import hashlib\n",
    "import logging\n",
    "#import mimetypes\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import urllib\n",
    "import urllib2\n",
    "import urlparse\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "from string import Template\n",
    "from urlparse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "#from pylons import config\n",
    "from owslib import wms\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "#from sqlalchemy import event\n",
    "#from sqlalchemy import distinct\n",
    "#from sqlalchemy import Table\n",
    "#from sqlalchemy import Column\n",
    "#from sqlalchemy import ForeignKey\n",
    "#from sqlalchemy import types\n",
    "#from sqlalchemy import Index\n",
    "#from sqlalchemy.engine.reflection import Inspector\n",
    "#from sqlalchemy.orm import backref, relation\n",
    "#from sqlalchemy.exc import InvalidRequestError\n",
    "#from sqlalchemy import exists\n",
    "#from sqlalchemy.sql import update, bindparam\n",
    "\n",
    "from ckan import plugins as p\n",
    "from ckan import model\n",
    "from ckan import logic\n",
    "\n",
    "#from ckan.model import Session\n",
    "#from ckan.model import Package\n",
    "#from ckan.model import PACKAGE_NAME_MAX_LENGTH\n",
    "#from ckan.model.meta import metadata\n",
    "from ckan.model.meta import mapper\n",
    "#from ckan.model.meta import Session\n",
    "from ckan.model.types import make_uuid\n",
    "from ckan.model.domain_object import DomainObject\n",
    "#from ckan.model.package import Package\n",
    "\n",
    "from ckan.plugins.interfaces import Interface\n",
    "from ckan.plugins.core import SingletonPlugin\n",
    "from ckan.plugins.core import implements\n",
    "\n",
    "from ckan.logic.schema import default_create_package_schema\n",
    "\n",
    "from ckan.lib.navl.validators import ignore_missing\n",
    "from ckan.lib.navl.validators import ignore\n",
    "from ckan.lib.navl.validators import not_empty\n",
    "from ckan.lib.munge import munge_title_to_name\n",
    "from ckan.lib.munge import substitute_ascii_equivalents\n",
    "#from ckan.lib.helpers import json\n",
    "#from ckan.lib.search.index import PackageSearchIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHarvester??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load E:\\GitHub\\ckan\\ckanext-harvest\\ckanext\\harvest\\interfaces.py\n",
    "\n",
    "\n",
    "class IHarvester(Interface):\n",
    "    '''\n",
    "    Common harvesting interface\n",
    "    '''\n",
    "    def info(self):\n",
    "        '''\n",
    "       place holder\n",
    "        '''\n",
    "    def validate_config(self, config):\n",
    "        '''\n",
    "       place holder\n",
    "        '''\n",
    "    def get_original_url(self, harvest_object_id):\n",
    "        '''\n",
    "        place holder\n",
    "        '''\n",
    "    def gather_stage(self, harvest_job):\n",
    "        '''\n",
    "        place holder\n",
    "        '''\n",
    "\n",
    "    def fetch_stage(self, harvest_object):\n",
    "        '''\n",
    "        place holder\n",
    "        '''\n",
    "    def import_stage(self, harvest_object):\n",
    "        '''\n",
    "        place holder\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Harvest model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load E:\\GitHub\\ckan\\ckanext-harvest\\ckanext\\harvest\\model\\__init__.py\n",
    "\n",
    "\n",
    "UPDATE_FREQUENCIES = ['MANUAL','MONTHLY','WEEKLY','BIWEEKLY','DAILY', 'ALWAYS']\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "__all__ = [\n",
    "    'HarvestSource', 'harvest_source_table',\n",
    "    'HarvestJob', 'harvest_job_table',\n",
    "    'HarvestObject', 'harvest_object_table',\n",
    "    'HarvestGatherError', 'harvest_gather_error_table',\n",
    "    'HarvestObjectError', 'harvest_object_error_table',\n",
    "    'HarvestLog', 'harvest_log_table'\n",
    "]\n",
    "\n",
    "\n",
    "harvest_source_table = None\n",
    "harvest_job_table = None\n",
    "harvest_object_table = None\n",
    "harvest_gather_error_table = None\n",
    "harvest_object_error_table = None\n",
    "harvest_object_extra_table = None\n",
    "harvest_log_table = None\n",
    "\n",
    "\n",
    "def setup():\n",
    "    pass\n",
    "\n",
    "    \"\"\"if harvest_source_table is None:\n",
    "        define_harvester_tables()\n",
    "        log.debug('Harvest tables defined in memory')\n",
    "\n",
    "    if not model.package_table.exists():\n",
    "        log.debug('Harvest table creation deferred')\n",
    "        return\n",
    "\n",
    "    if not harvest_source_table.exists():\n",
    "\n",
    "        # Create each table individually rather than\n",
    "        # using metadata.create_all()\n",
    "        harvest_source_table.create()\n",
    "        harvest_job_table.create()\n",
    "        harvest_object_table.create()\n",
    "        harvest_gather_error_table.create()\n",
    "        harvest_object_error_table.create()\n",
    "        harvest_object_extra_table.create()\n",
    "        harvest_log_table.create()\n",
    "        \n",
    "        log.debug('Harvest tables created')\n",
    "    else:\n",
    "        from ckan.model.meta import engine\n",
    "        log.debug('Harvest tables already exist')\n",
    "        # Check if existing tables need to be updated\n",
    "        inspector = Inspector.from_engine(engine)\n",
    "        columns = inspector.get_columns('harvest_source')\n",
    "        column_names = [column['name'] for column in columns]\n",
    "        if not 'title' in column_names:\n",
    "            log.debug('Harvest tables need to be updated')\n",
    "            migrate_v2()\n",
    "        if not 'frequency' in column_names:\n",
    "            log.debug('Harvest tables need to be updated')\n",
    "            migrate_v3()\n",
    "\n",
    "        # Check if this instance has harvest source datasets\n",
    "        source_ids = Session.query(HarvestSource.id).filter_by(active=True).all()\n",
    "        source_package_ids = Session.query(model.Package.id).filter_by(type=u'harvest', state='active').all()\n",
    "        sources_to_migrate = set(source_ids) - set(source_package_ids)\n",
    "        if sources_to_migrate:\n",
    "            log.debug('Creating harvest source datasets for %i existing sources', len(sources_to_migrate))\n",
    "            sources_to_migrate = [s[0] for s in sources_to_migrate]\n",
    "            migrate_v3_create_datasets(sources_to_migrate)\n",
    "            \n",
    "        # Check if harvest_log table exist - needed for existing users\n",
    "        if not 'harvest_log' in inspector.get_table_names():\n",
    "            harvest_log_table.create()\n",
    "\n",
    "        # Check if harvest_object has a index\n",
    "        index_names = [index['name'] for index in inspector.get_indexes(\"harvest_object\")]\n",
    "        if not \"harvest_job_id_idx\" in index_names:\n",
    "            log.debug('Creating index for harvest_object')\n",
    "            Index(\"harvest_job_id_idx\", harvest_object_table.c.harvest_job_id).create()\"\"\"\n",
    "\n",
    "\n",
    "class HarvestError(Exception):\n",
    "    pass\n",
    "\n",
    "class HarvestDomainObject(DomainObject):\n",
    "    '''Convenience methods for searching objects\n",
    "    '''\n",
    "    key_attr = 'id'\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, key, default=None, attr=None):\n",
    "        '''Finds a single entity in the register.'''\n",
    "        if attr == None:\n",
    "            attr = cls.key_attr\n",
    "        kwds = {attr: key}\n",
    "        o = cls.filter(**kwds).first()\n",
    "        if o:\n",
    "            return o\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    @classmethod\n",
    "    def filter(cls, **kwds):\n",
    "        query = Session.query(cls).autoflush(False)\n",
    "        return query.filter_by(**kwds)\n",
    "\n",
    "\n",
    "class HarvestSource(HarvestDomainObject):\n",
    "    '''A Harvest Source is essentially a URL plus some other metadata.\n",
    "       It must have a type (e.g. CSW) and can have a status of \"active\"\n",
    "       or \"inactive\". The harvesting processes are not fired on inactive\n",
    "       sources.\n",
    "    '''\n",
    "    def __repr__(self):\n",
    "        return '<HarvestSource id=%s title=%s url=%s active=%r>' % \\\n",
    "               (self.id, self.title, self.url, self.active)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__().encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "class HarvestJob(HarvestDomainObject):\n",
    "    '''A Harvesting Job is performed in two phases. In first place, the\n",
    "       **gather** stage collects all the Ids and URLs that need to be fetched\n",
    "       from the harvest source. Errors occurring in this phase\n",
    "       (``HarvestGatherError``) are stored in the ``harvest_gather_error``\n",
    "       table. During the next phase, the **fetch** stage retrieves the\n",
    "       ``HarvestedObjects`` and, if necessary, the **import** stage stores\n",
    "       them on the database. Errors occurring in this second stage\n",
    "       (``HarvestObjectError``) are stored in the ``harvest_object_error``\n",
    "       table.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class HarvestObject(HarvestDomainObject):\n",
    "    '''A Harvest Object is created every time an element is fetched from a\n",
    "       harvest source. Its contents can be processed and imported to ckan\n",
    "       packages, RDF graphs, etc.\n",
    "\n",
    "    '''\n",
    "\n",
    "class HarvestObjectExtra(HarvestDomainObject):\n",
    "    '''Extra key value data for Harvest objects'''\n",
    "\n",
    "class HarvestGatherError(HarvestDomainObject):\n",
    "    '''Gather errors are raised during the **gather** stage of a harvesting\n",
    "       job.\n",
    "    '''\n",
    "    @classmethod\n",
    "    def create(cls, message, job):\n",
    "        '''\n",
    "        Helper function to create an error object and save it.\n",
    "        '''\n",
    "        err = cls(message=message, job=job)\n",
    "        try:\n",
    "            err.save()\n",
    "        except InvalidRequestError:\n",
    "            Session.rollback()\n",
    "            err.save()\n",
    "        finally:\n",
    "            # No need to alert administrator so don't log as an error\n",
    "            log.info(message)\n",
    "\n",
    "\n",
    "class HarvestObjectError(HarvestDomainObject):\n",
    "    '''Object errors are raised during the **fetch** or **import** stage of a\n",
    "       harvesting job, and are referenced to a specific harvest object.\n",
    "    '''\n",
    "    @classmethod\n",
    "    def create(cls, message, object, stage=u'Fetch', line=None):\n",
    "        '''\n",
    "        Helper function to create an error object and save it.\n",
    "        '''\n",
    "        err = cls(message=message, object=object,\n",
    "                  stage=stage, line=line)\n",
    "        try:\n",
    "            err.save()\n",
    "        except InvalidRequestError, e:\n",
    "            # Clear any in-progress sqlalchemy transactions\n",
    "            try:\n",
    "                Session.rollback()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                Session.remove()\n",
    "            except:\n",
    "                pass\n",
    "            err.save()\n",
    "        finally:\n",
    "            log_message = '{0}, line {1}'.format(message, line) \\\n",
    "                          if line else message\n",
    "            log.debug(log_message)\n",
    "\n",
    "class HarvestLog(HarvestDomainObject):\n",
    "    '''HarvestLog objects are created each time something is logged\n",
    "       using python's standard logging module\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def harvest_object_before_insert_listener(mapper,connection,target):\n",
    "    '''\n",
    "        For compatibility with old harvesters, check if the source id has\n",
    "        been set, and set it automatically from the job if not.\n",
    "    '''\n",
    "    if not target.harvest_source_id or not target.source:\n",
    "        if not target.job:\n",
    "            raise Exception('You must define a Harvest Job for each Harvest Object')\n",
    "        target.source = target.job.source\n",
    "        target.harvest_source_id = target.job.source.id\n",
    "\n",
    "\n",
    "def define_harvester_tables():\n",
    "    pass\n",
    "\n",
    "    \"\"\" \n",
    "remove content\n",
    "    \"\"\"\n",
    "\n",
    "def migrate_v2():\n",
    "    pass\n",
    "    \"\"\"\n",
    "remove content--not using\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def migrate_v3():\n",
    "    pass\n",
    "    \"\"\"\n",
    "    remove content, not using\n",
    "    \"\"\"\n",
    "\n",
    "class PackageIdHarvestSourceIdMismatch(Exception):\n",
    "    \"\"\"\n",
    "    The package created for the harvest source must match the id of the\n",
    "    harvest source\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def migrate_v3_create_datasets(source_ids=None):\n",
    "    pass\n",
    "    \"\"\"\n",
    "    remove content\n",
    "    \"\"\"\n",
    "\n",
    "def clean_harvest_log(condition):\n",
    "    pass\n",
    "    \"\"\"\n",
    "    remove content\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Harvest base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load E:\\GitHub\\ckan\\ckanext-harvest\\ckanext\\harvest\\harvesters\\base.py\n",
    "\n",
    "#smr from ckanext.harvest.model import (HarvestObject, HarvestGatherError, HarvestObjectError, HarvestJob)\n",
    "#import HarvestObject, HarvestGatherError, HarvestObjectError, HarvestJob\n",
    "\n",
    "\n",
    "#from ckanext.harvest.interfaces import IHarvester\n",
    "\"\"\"\n",
    "SMR comment this out and use the else option; this introduces dependency on repoze.who.config\n",
    "which doesn't want to pip install\n",
    "\n",
    "if p.toolkit.check_ckan_version(min_version='2.3'):\n",
    "    from ckan.lib.munge import munge_tag\n",
    "else:\n",
    "    # Fallback munge_tag for older ckan versions which don't have a decent\n",
    "    # munger\n",
    "    \n",
    "\"\"\"   \n",
    "    \n",
    "def _munge_to_length(string, min_length, max_length):\n",
    "    '''Pad/truncates a string'''\n",
    "    if len(string) < min_length:\n",
    "        string += '_' * (min_length - len(string))\n",
    "    if len(string) > max_length:\n",
    "        string = string[:max_length]\n",
    "    return string\n",
    "\n",
    "def munge_tag(tag):\n",
    "    tag = substitute_ascii_equivalents(tag)\n",
    "    tag = tag.lower().strip()\n",
    "    tag = re.sub(r'[^a-zA-Z0-9\\- ]', '', tag).replace(' ', '-')\n",
    "    tag = _munge_to_length(tag, model.MIN_TAG_LENGTH, model.MAX_TAG_LENGTH)\n",
    "    return tag\n",
    "\n",
    "# end SMR adjustment\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class HarvesterBase(SingletonPlugin):\n",
    "    '''\n",
    "    Generic base class for harvesters, providing a number of useful functions.\n",
    "\n",
    "    A harvester doesn't have to derive from this - it could just have:\n",
    "\n",
    "        implements(IHarvester)\n",
    "    '''\n",
    "    implements(IHarvester)\n",
    "\n",
    "    config = None\n",
    "\n",
    "    _user_name = None\n",
    "\n",
    "    @classmethod\n",
    "    def _gen_new_name(cls, title, existing_name=None,\n",
    "                      append_type=None):\n",
    "        '''\n",
    "        Returns a 'name' for the dataset (URL friendly), based on the title.\n",
    "\n",
    "        If the ideal name is already used, it will append a number to it to\n",
    "        ensure it is unique.\n",
    "\n",
    "        If generating a new name because the title of the dataset has changed,\n",
    "        specify the existing name, in case the name doesn't need to change\n",
    "        after all.\n",
    "\n",
    "        :param existing_name: the current name of the dataset - only specify\n",
    "                              this if the dataset exists\n",
    "        :type existing_name: string\n",
    "        :param append_type: the type of characters to add to make it unique -\n",
    "                            either 'number-sequence' or 'random-hex'.\n",
    "        :type append_type: string\n",
    "        '''\n",
    "\n",
    "        # If append_type was given, use it. Otherwise, use the configured default.\n",
    "        # If nothing was given and no defaults were set, use 'number-sequence'.\n",
    "        if append_type:\n",
    "            append_type_param = append_type\n",
    "        else:\n",
    "            append_type_param = config.get('ckanext.harvest.default_dataset_name_append',\n",
    "                                           'number-sequence')\n",
    "\n",
    "        ideal_name = munge_title_to_name(title)\n",
    "        ideal_name = re.sub('-+', '-', ideal_name)  # collapse multiple dashes\n",
    "        return cls._ensure_name_is_unique(ideal_name,\n",
    "                                          existing_name=existing_name,\n",
    "                                          append_type=append_type_param)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_name_is_unique(ideal_name, existing_name=None,\n",
    "                               append_type='number-sequence'):\n",
    "        '''\n",
    "        Returns a dataset name based on the ideal_name, only it will be\n",
    "        guaranteed to be different than all the other datasets, by adding a\n",
    "        number on the end if necessary.\n",
    "\n",
    "        If generating a new name because the title of the dataset has changed,\n",
    "        specify the existing name, in case the name doesn't need to change\n",
    "        after all.\n",
    "\n",
    "        The maximum dataset name length is taken account of.\n",
    "\n",
    "        :param ideal_name: the desired name for the dataset, if its not already\n",
    "                           been taken (usually derived by munging the dataset\n",
    "                           title)\n",
    "        :type ideal_name: string\n",
    "        :param existing_name: the current name of the dataset - only specify\n",
    "                              this if the dataset exists\n",
    "        :type existing_name: string\n",
    "        :param append_type: the type of characters to add to make it unique -\n",
    "                            either 'number-sequence' or 'random-hex'.\n",
    "        :type append_type: string\n",
    "        '''\n",
    "        ideal_name = ideal_name[:PACKAGE_NAME_MAX_LENGTH]\n",
    "        if existing_name == ideal_name:\n",
    "            return ideal_name\n",
    "        if append_type == 'number-sequence':\n",
    "            MAX_NUMBER_APPENDED = 999\n",
    "            APPEND_MAX_CHARS = len(str(MAX_NUMBER_APPENDED))\n",
    "        elif append_type == 'random-hex':\n",
    "            APPEND_MAX_CHARS = 5  # 16^5 = 1 million combinations\n",
    "        else:\n",
    "            raise NotImplementedError('append_type cannot be %s' % append_type)\n",
    "        # Find out which package names have been taken. Restrict it to names\n",
    "        # derived from the ideal name plus and numbers added\n",
    "        like_q = u'%s%%' % \\\n",
    "            ideal_name[:PACKAGE_NAME_MAX_LENGTH-APPEND_MAX_CHARS]\n",
    "        name_results = Session.query(Package.name)\\\n",
    "                              .filter(Package.name.ilike(like_q))\\\n",
    "                              .all()\n",
    "        taken = set([name_result[0] for name_result in name_results])\n",
    "        if existing_name and existing_name in taken:\n",
    "            taken.remove(existing_name)\n",
    "        if ideal_name not in taken:\n",
    "            # great, the ideal name is available\n",
    "            return ideal_name\n",
    "        elif existing_name and existing_name.startswith(ideal_name):\n",
    "            # the ideal name is not available, but its an existing dataset with\n",
    "            # a name based on the ideal one, so there's no point changing it to\n",
    "            # a different number\n",
    "            return existing_name\n",
    "        elif append_type == 'number-sequence':\n",
    "            # find the next available number\n",
    "            counter = 1\n",
    "            while counter <= MAX_NUMBER_APPENDED:\n",
    "                candidate_name = \\\n",
    "                    ideal_name[:PACKAGE_NAME_MAX_LENGTH-len(str(counter))] + \\\n",
    "                    str(counter)\n",
    "                if candidate_name not in taken:\n",
    "                    return candidate_name\n",
    "                counter = counter + 1\n",
    "            return None\n",
    "        elif append_type == 'random-hex':\n",
    "            return ideal_name[:PACKAGE_NAME_MAX_LENGTH-APPEND_MAX_CHARS] + \\\n",
    "                str(uuid.uuid4())[:APPEND_MAX_CHARS]\n",
    "\n",
    "    _save_gather_error = HarvestGatherError.create\n",
    "    _save_object_error = HarvestObjectError.create\n",
    "\n",
    "    def _get_user_name(self):\n",
    "        '''\n",
    "        Returns the name of the user that will perform the harvesting actions\n",
    "        (deleting, updating and creating datasets)\n",
    "\n",
    "        By default this will be the old 'harvest' user to maintain\n",
    "        compatibility. If not present, the internal site admin user will be\n",
    "        used. This is the recommended setting, but if necessary it can be\n",
    "        overridden with the `ckanext.harvest.user_name` config option:\n",
    "\n",
    "           ckanext.harvest.user_name = harvest\n",
    "\n",
    "        '''\n",
    "        if self._user_name:\n",
    "            return self._user_name\n",
    "\n",
    "        config_user_name = config.get('ckanext.harvest.user_name')\n",
    "        if config_user_name:\n",
    "            self._user_name = config_user_name\n",
    "            return self._user_name\n",
    "\n",
    "        context = {'model': model,\n",
    "                   'ignore_auth': True,\n",
    "                   }\n",
    "\n",
    "        # Check if 'harvest' user exists and if is a sysadmin\n",
    "        try:\n",
    "            user_harvest = p.toolkit.get_action('user_show')(\n",
    "                context, {'id': 'harvest'})\n",
    "            if user_harvest['sysadmin']:\n",
    "                self._user_name = 'harvest'\n",
    "                return self._user_name\n",
    "        except p.toolkit.ObjectNotFound:\n",
    "            pass\n",
    "\n",
    "        context['defer_commit'] = True  # See ckan/ckan#1714\n",
    "        self._site_user = p.toolkit.get_action('get_site_user')(context, {})\n",
    "        self._user_name = self._site_user['name']\n",
    "\n",
    "        return self._user_name\n",
    "\n",
    "    def _create_harvest_objects(self, remote_ids, harvest_job):\n",
    "        '''\n",
    "        Given a list of remote ids and a Harvest Job, create as many Harvest Objects and\n",
    "        return a list of their ids to be passed to the fetch stage.\n",
    "\n",
    "        TODO: Not sure it is worth keeping this function\n",
    "        '''\n",
    "        try:\n",
    "            object_ids = []\n",
    "            if len(remote_ids):\n",
    "                for remote_id in remote_ids:\n",
    "                    # Create a new HarvestObject for this identifier\n",
    "                    obj = HarvestObject(guid = remote_id, job = harvest_job)\n",
    "                    obj.save()\n",
    "                    object_ids.append(obj.id)\n",
    "                return object_ids\n",
    "            else:\n",
    "               self._save_gather_error('No remote datasets could be identified', harvest_job)\n",
    "        except Exception, e:\n",
    "            self._save_gather_error('%r' % e.message, harvest_job)\n",
    "\n",
    "    def _create_or_update_package(self, package_dict, harvest_object,\n",
    "                                  package_dict_form='rest'):\n",
    "        '''\n",
    "        Creates a new package or updates an existing one according to the\n",
    "        package dictionary provided.\n",
    "\n",
    "        The package dictionary can be in one of two forms:\n",
    "\n",
    "        1. 'rest' - as seen on the RESTful API:\n",
    "\n",
    "                http://datahub.io/api/rest/dataset/1996_population_census_data_canada\n",
    "\n",
    "           This is the legacy form. It is the default to provide backward\n",
    "           compatibility.\n",
    "\n",
    "           * 'extras' is a dict e.g. {'theme': 'health', 'sub-theme': 'cancer'}\n",
    "           * 'tags' is a list of strings e.g. ['large-river', 'flood']\n",
    "\n",
    "        2. 'package_show' form, as provided by the Action API (CKAN v2.0+):\n",
    "\n",
    "               http://datahub.io/api/action/package_show?id=1996_population_census_data_canada\n",
    "\n",
    "           * 'extras' is a list of dicts\n",
    "                e.g. [{'key': 'theme', 'value': 'health'},\n",
    "                        {'key': 'sub-theme', 'value': 'cancer'}]\n",
    "           * 'tags' is a list of dicts\n",
    "                e.g. [{'name': 'large-river'}, {'name': 'flood'}]\n",
    "\n",
    "        Note that the package_dict must contain an id, which will be used to\n",
    "        check if the package needs to be created or updated (use the remote\n",
    "        dataset id).\n",
    "\n",
    "        If the remote server provides the modification date of the remote\n",
    "        package, add it to package_dict['metadata_modified'].\n",
    "\n",
    "        :returns: The same as what import_stage should return. i.e. True if the\n",
    "                  create or update occurred ok, 'unchanged' if it didn't need\n",
    "                  updating or False if there were errors.\n",
    "\n",
    "\n",
    "        TODO: Not sure it is worth keeping this function. If useful it should\n",
    "        use the output of package_show logic function (maybe keeping support\n",
    "        for rest api based dicts\n",
    "        '''\n",
    "        assert package_dict_form in ('rest', 'package_show')\n",
    "        try:\n",
    "            # Change default schema\n",
    "            schema = default_create_package_schema()\n",
    "            schema['id'] = [ignore_missing, unicode]\n",
    "            schema['__junk'] = [ignore]\n",
    "\n",
    "            # Check API version\n",
    "            if self.config:\n",
    "                try:\n",
    "                    api_version = int(self.config.get('api_version', 2))\n",
    "                except ValueError:\n",
    "                    raise ValueError('api_version must be an integer')\n",
    "            else:\n",
    "                api_version = 2\n",
    "\n",
    "            user_name = self._get_user_name()\n",
    "            context = {\n",
    "                'model': model,\n",
    "                'session': Session,\n",
    "                'user': user_name,\n",
    "                'api_version': api_version,\n",
    "                'schema': schema,\n",
    "                'ignore_auth': True,\n",
    "            }\n",
    "\n",
    "            if self.config and self.config.get('clean_tags', False):\n",
    "                tags = package_dict.get('tags', [])\n",
    "                package_dict['tags'] = self._clean_tags(tags)\n",
    "\n",
    "            # Check if package exists\n",
    "            try:\n",
    "                # _find_existing_package can be overridden if necessary\n",
    "                existing_package_dict = self._find_existing_package(package_dict)\n",
    "\n",
    "                # In case name has been modified when first importing. See issue #101.\n",
    "                package_dict['name'] = existing_package_dict['name']\n",
    "\n",
    "                # Check modified date\n",
    "                if not 'metadata_modified' in package_dict or \\\n",
    "                   package_dict['metadata_modified'] > existing_package_dict.get('metadata_modified'):\n",
    "                    log.info('Package with GUID %s exists and needs to be updated' % harvest_object.guid)\n",
    "                    # Update package\n",
    "                    context.update({'id':package_dict['id']})\n",
    "                    package_dict.setdefault('name',\n",
    "                                            existing_package_dict['name'])\n",
    "\n",
    "                    new_package = p.toolkit.get_action(\n",
    "                        'package_update' if package_dict_form == 'package_show'\n",
    "                        else 'package_update_rest')(context, package_dict)\n",
    "\n",
    "                else:\n",
    "                    log.info('No changes to package with GUID %s, skipping...' % harvest_object.guid)\n",
    "                    # NB harvest_object.current/package_id are not set\n",
    "                    return 'unchanged'\n",
    "\n",
    "                # Flag the other objects linking to this package as not current anymore\n",
    "                from ckanext.harvest.model import harvest_object_table\n",
    "                conn = Session.connection()\n",
    "                u = update(harvest_object_table) \\\n",
    "                        .where(harvest_object_table.c.package_id==bindparam('b_package_id')) \\\n",
    "                        .values(current=False)\n",
    "                conn.execute(u, b_package_id=new_package['id'])\n",
    "\n",
    "                # Flag this as the current harvest object\n",
    "\n",
    "                harvest_object.package_id = new_package['id']\n",
    "                harvest_object.current = True\n",
    "                harvest_object.save()\n",
    "\n",
    "            except p.toolkit.ObjectNotFound:\n",
    "                # Package needs to be created\n",
    "\n",
    "                # Get rid of auth audit on the context otherwise we'll get an\n",
    "                # exception\n",
    "                context.pop('__auth_audit', None)\n",
    "\n",
    "                # Set name for new package to prevent name conflict, see issue #117\n",
    "                if package_dict.get('name', None):\n",
    "                    package_dict['name'] = self._gen_new_name(package_dict['name'])\n",
    "                else:\n",
    "                    package_dict['name'] = self._gen_new_name(package_dict['title'])\n",
    "\n",
    "                log.info('Package with GUID %s does not exist, let\\'s create it' % harvest_object.guid)\n",
    "                harvest_object.current = True\n",
    "                harvest_object.package_id = package_dict['id']\n",
    "                # Defer constraints and flush so the dataset can be indexed with\n",
    "                # the harvest object id (on the after_show hook from the harvester\n",
    "                # plugin)\n",
    "                harvest_object.add()\n",
    "\n",
    "                model.Session.execute('SET CONSTRAINTS harvest_object_package_id_fkey DEFERRED')\n",
    "                model.Session.flush()\n",
    "\n",
    "                new_package = p.toolkit.get_action(\n",
    "                    'package_create' if package_dict_form == 'package_show'\n",
    "                    else 'package_create_rest')(context, package_dict)\n",
    "\n",
    "            Session.commit()\n",
    "\n",
    "            return True\n",
    "\n",
    "        except p.toolkit.ValidationError, e:\n",
    "            log.exception(e)\n",
    "            self._save_object_error('Invalid package with GUID %s: %r'%(harvest_object.guid,e.error_dict),harvest_object,'Import')\n",
    "        except Exception, e:\n",
    "            log.exception(e)\n",
    "            self._save_object_error('%r'%e,harvest_object,'Import')\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _find_existing_package(self, package_dict):\n",
    "        data_dict = {'id': package_dict['id']}\n",
    "        package_show_context = {'model': model, 'session': Session,\n",
    "                                'ignore_auth': True}\n",
    "        return p.toolkit.get_action('package_show')(\n",
    "            package_show_context, data_dict)\n",
    "\n",
    "    def _clean_tags(self, tags):\n",
    "        try:\n",
    "            def _update_tag(tag_dict, key, newvalue):\n",
    "                # update the dict and return it\n",
    "                tag_dict[key] = newvalue\n",
    "                return tag_dict\n",
    "                                \n",
    "            # assume it's in the package_show form                    \n",
    "            tags = [_update_tag(t, 'name', munge_tag(t['name'])) for t in tags if munge_tag(t['name']) != '']\n",
    "\n",
    "        except TypeError: # a TypeError is raised if `t` above is a string\n",
    "           # REST format: 'tags' is a list of strings\n",
    "           tags = [munge_tag(t) for t in tags if munge_tag(t) != '']                \n",
    "           tags = list(set(tags))\n",
    "           return tags\n",
    "           \n",
    "        return tags      \n",
    "\n",
    "    @classmethod\n",
    "    def last_error_free_job(cls, harvest_job):\n",
    "        # TODO weed out cancelled jobs somehow.\n",
    "        # look for jobs with no gather errors\n",
    "        jobs = \\\n",
    "            model.Session.query(HarvestJob) \\\n",
    "                 .filter(HarvestJob.source == harvest_job.source) \\\n",
    "                 .filter(HarvestJob.gather_started != None) \\\n",
    "                 .filter(HarvestJob.status == 'Finished') \\\n",
    "                 .filter(HarvestJob.id != harvest_job.id) \\\n",
    "                 .filter(\n",
    "                     ~exists().where(\n",
    "                         HarvestGatherError.harvest_job_id == HarvestJob.id)) \\\n",
    "                 .order_by(HarvestJob.gather_started.desc())\n",
    "        # now check them until we find one with no fetch/import errors\n",
    "        # (looping rather than doing sql, in case there are lots of objects\n",
    "        # and lots of jobs)\n",
    "        for job in jobs:\n",
    "            for obj in job.objects:\n",
    "                if obj.current is False and \\\n",
    "                        obj.report_status != 'not modified':\n",
    "                    # unsuccessful, so go onto the next job\n",
    "                    break\n",
    "            else:\n",
    "                return job\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ISO XML metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model harvested metadata \n",
    "\n",
    "# %load E:\\GitHub\\ckan\\ckanext-spatial\\ckanext\\spatial\\model\\harvested_metadata.py\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MappedXmlObject(object):\n",
    "    elements = []\n",
    "\n",
    "\n",
    "class MappedXmlDocument(MappedXmlObject):\n",
    "    def __init__(self, xml_str=None, xml_tree=None):\n",
    "        assert (xml_str or xml_tree is not None), 'Must provide some XML in one format or another'\n",
    "        self.xml_str = xml_str\n",
    "        self.xml_tree = xml_tree\n",
    "\n",
    "    def read_values(self):\n",
    "        '''For all of the elements listed, finds the values of them in the\n",
    "        XML and returns them.'''\n",
    "        values = {}\n",
    "        tree = self.get_xml_tree()\n",
    "        for element in self.elements:\n",
    "            values[element.name] = element.read_value(tree)\n",
    "        self.infer_values(values)\n",
    "        return values\n",
    "\n",
    "    def read_value(self, name):\n",
    "        '''For the given element name, find the value in the XML and return\n",
    "        it.\n",
    "        '''\n",
    "        tree = self.get_xml_tree()\n",
    "        for element in self.elements:\n",
    "            if element.name == name:\n",
    "                return element.read_value(tree)\n",
    "        raise KeyError\n",
    "\n",
    "    def get_xml_tree(self):\n",
    "        if self.xml_tree is None:\n",
    "            parser = etree.XMLParser(remove_blank_text=True)\n",
    "            if type(self.xml_str) == unicode:\n",
    "                xml_str = self.xml_str.encode('utf8')\n",
    "            else:\n",
    "                xml_str = self.xml_str\n",
    "            self.xml_tree = etree.fromstring(xml_str, parser=parser)\n",
    "        return self.xml_tree\n",
    "\n",
    "    def infer_values(self, values):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MappedXmlElement(MappedXmlObject):\n",
    "    namespaces = {}\n",
    "\n",
    "    def __init__(self, name, search_paths=[], multiplicity=\"*\", elements=[]):\n",
    "        self.name = name\n",
    "        self.search_paths = search_paths\n",
    "        self.multiplicity = multiplicity\n",
    "        self.elements = elements or self.elements\n",
    "\n",
    "    def read_value(self, tree):\n",
    "        values = []\n",
    "        for xpath in self.get_search_paths():\n",
    "            elements = self.get_elements(tree, xpath)\n",
    "            values = self.get_values(elements)\n",
    "            if values:\n",
    "                break\n",
    "        return self.fix_multiplicity(values)\n",
    "\n",
    "    def get_search_paths(self):\n",
    "        if type(self.search_paths) != type([]):\n",
    "            search_paths = [self.search_paths]\n",
    "        else:\n",
    "            search_paths = self.search_paths\n",
    "        return search_paths\n",
    "\n",
    "    def get_elements(self, tree, xpath):\n",
    "        return tree.xpath(xpath, namespaces=self.namespaces)\n",
    "\n",
    "    def get_values(self, elements):\n",
    "        values = []\n",
    "        if len(elements) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for element in elements:\n",
    "                value = self.get_value(element)\n",
    "                values.append(value)\n",
    "        return values\n",
    "\n",
    "    def get_value(self, element):\n",
    "        if self.elements:\n",
    "            value = {}\n",
    "            for child in self.elements:\n",
    "                value[child.name] = child.read_value(element)\n",
    "            return value\n",
    "        elif type(element) == etree._ElementStringResult:\n",
    "            value = str(element)\n",
    "        elif type(element) == etree._ElementUnicodeResult:\n",
    "            value = unicode(element)\n",
    "        else:\n",
    "            value = self.element_tostring(element)\n",
    "        return value\n",
    "\n",
    "    def element_tostring(self, element):\n",
    "        return etree.tostring(element, pretty_print=False)\n",
    "\n",
    "    def fix_multiplicity(self, values):\n",
    "        '''\n",
    "        When a field contains multiple values, yet the spec says\n",
    "        it should contain only one, then return just the first value,\n",
    "        rather than a list.\n",
    "\n",
    "        In the ISO19115 specification, multiplicity relates to:\n",
    "        * 'Association Cardinality'\n",
    "        * 'Obligation/Condition' & 'Maximum Occurence'\n",
    "        '''\n",
    "        if self.multiplicity == \"0\":\n",
    "            # 0 = None\n",
    "            if values:\n",
    "                log.warn(\"Values found for element '%s' when multiplicity should be 0: %s\",  self.name, values)\n",
    "            return \"\"\n",
    "        elif self.multiplicity == \"1\":\n",
    "            # 1 = Mandatory, maximum 1 = Exactly one\n",
    "            if not values:\n",
    "                log.warn(\"Value not found for element '%s'\" % self.name)\n",
    "                return ''\n",
    "            return values[0]\n",
    "        elif self.multiplicity == \"*\":\n",
    "            # * = 0..* = zero or more\n",
    "            return values\n",
    "        elif self.multiplicity == \"0..1\":\n",
    "            # 0..1 = Mandatory, maximum 1 = optional (zero or one)\n",
    "            if values:\n",
    "                return values[0]\n",
    "            else:\n",
    "                return \"\"\n",
    "        elif self.multiplicity == \"1..*\":\n",
    "            # 1..* = one or more\n",
    "            return values\n",
    "        else:\n",
    "            log.warning('Multiplicity not specified for element: %s',\n",
    "                        self.name)\n",
    "            return values\n",
    "\n",
    "\n",
    "class ISOElement(MappedXmlElement):\n",
    "\n",
    "    namespaces = {\n",
    "       \"gts\": \"http://www.isotc211.org/2005/gts\",\n",
    "       \"gml\": \"http://www.opengis.net/gml\",\n",
    "       \"gml32\": \"http://www.opengis.net/gml/3.2\",\n",
    "       \"gmx\": \"http://www.isotc211.org/2005/gmx\",\n",
    "       \"gsr\": \"http://www.isotc211.org/2005/gsr\",\n",
    "       \"gss\": \"http://www.isotc211.org/2005/gss\",\n",
    "       \"gco\": \"http://www.isotc211.org/2005/gco\",\n",
    "       \"gmd\": \"http://www.isotc211.org/2005/gmd\",\n",
    "       \"srv\": \"http://www.isotc211.org/2005/srv\",\n",
    "       \"xlink\": \"http://www.w3.org/1999/xlink\",\n",
    "       \"xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "    }\n",
    "\n",
    "\n",
    "class ISOResourceLocator(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"url\",\n",
    "            search_paths=[\n",
    "                \"gmd:linkage/gmd:URL/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"function\",\n",
    "            search_paths=[\n",
    "                \"gmd:function/gmd:CI_OnLineFunctionCode/@codeListValue\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"name\",\n",
    "            search_paths=[\n",
    "                \"gmd:name/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"description\",\n",
    "            search_paths=[\n",
    "                \"gmd:description/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"protocol\",\n",
    "            search_paths=[\n",
    "                \"gmd:protocol/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ]\n",
    "\n",
    "\n",
    "class ISOResponsibleParty(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"individual-name\",\n",
    "            search_paths=[\n",
    "                \"gmd:individualName/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"organisation-name\",\n",
    "            search_paths=[\n",
    "                \"gmd:organisationName/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"position-name\",\n",
    "            search_paths=[\n",
    "                \"gmd:positionName/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"contact-info\",\n",
    "            search_paths=[\n",
    "                \"gmd:contactInfo/gmd:CI_Contact\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "            elements = [\n",
    "                ISOElement(\n",
    "                    name=\"email\",\n",
    "                    search_paths=[\n",
    "                        \"gmd:address/gmd:CI_Address/gmd:electronicMailAddress/gco:CharacterString/text()\",\n",
    "                    ],\n",
    "                    multiplicity=\"0..1\",\n",
    "                ),\n",
    "                ISOResourceLocator(\n",
    "                    name=\"online-resource\",\n",
    "                    search_paths=[\n",
    "                        \"gmd:onlineResource/gmd:CI_OnlineResource\",\n",
    "                    ],\n",
    "                    multiplicity=\"0..1\",\n",
    "                ),\n",
    "\n",
    "            ]\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"role\",\n",
    "            search_paths=[\n",
    "                \"gmd:role/gmd:CI_RoleCode/@codeListValue\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "class ISODataFormat(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"name\",\n",
    "            search_paths=[\n",
    "                \"gmd:name/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"version\",\n",
    "            search_paths=[\n",
    "                \"gmd:version/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "class ISOReferenceDate(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"type\",\n",
    "            search_paths=[\n",
    "                \"gmd:dateType/gmd:CI_DateTypeCode/@codeListValue\",\n",
    "                \"gmd:dateType/gmd:CI_DateTypeCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"value\",\n",
    "            search_paths=[\n",
    "                \"gmd:date/gco:Date/text()\",\n",
    "                \"gmd:date/gco:DateTime/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "class ISOCoupledResources(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"title\",\n",
    "            search_paths=[\n",
    "                \"@xlink:title\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"href\",\n",
    "            search_paths=[\n",
    "                \"@xlink:href\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"uuid\",\n",
    "            search_paths=[\n",
    "                \"@uuidref\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "class ISOBoundingBox(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"west\",\n",
    "            search_paths=[\n",
    "                \"gmd:westBoundLongitude/gco:Decimal/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"east\",\n",
    "            search_paths=[\n",
    "                \"gmd:eastBoundLongitude/gco:Decimal/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"north\",\n",
    "            search_paths=[\n",
    "                \"gmd:northBoundLatitude/gco:Decimal/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"south\",\n",
    "            search_paths=[\n",
    "                \"gmd:southBoundLatitude/gco:Decimal/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "class ISOBrowseGraphic(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"file\",\n",
    "            search_paths=[\n",
    "                \"gmd:fileName/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"description\",\n",
    "            search_paths=[\n",
    "                \"gmd:fileDescription/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"type\",\n",
    "            search_paths=[\n",
    "                \"gmd:fileType/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "class ISOKeyword(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"keyword\",\n",
    "            search_paths=[\n",
    "                \"gmd:keyword/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"type\",\n",
    "            search_paths=[\n",
    "                \"gmd:type/gmd:MD_KeywordTypeCode/@codeListValue\",\n",
    "                \"gmd:type/gmd:MD_KeywordTypeCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        # If Thesaurus information is needed at some point, this is the\n",
    "        # place to add it\n",
    "   ]\n",
    "\n",
    "\n",
    "class ISOUsage(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"usage\",\n",
    "            search_paths=[\n",
    "                \"gmd:specificUsage/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOResponsibleParty(\n",
    "            name=\"contact-info\",\n",
    "            search_paths=[\n",
    "                \"gmd:userContactInfo/gmd:CI_ResponsibleParty\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "\n",
    "   ]\n",
    "\n",
    "\n",
    "class ISOAggregationInfo(ISOElement):\n",
    "\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"aggregate-dataset-name\",\n",
    "            search_paths=[\n",
    "                \"gmd:aggregateDatasetName/gmd:CI_Citation/gmd:title/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"aggregate-dataset-identifier\",\n",
    "            search_paths=[\n",
    "                \"gmd:aggregateDatasetIdentifier/gmd:MD_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"association-type\",\n",
    "            search_paths=[\n",
    "                \"gmd:associationType/gmd:DS_AssociationTypeCode/@codeListValue\",\n",
    "                \"gmd:associationType/gmd:DS_AssociationTypeCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"initiative-type\",\n",
    "            search_paths=[\n",
    "                \"gmd:initiativeType/gmd:DS_InitiativeTypeCode/@codeListValue\",\n",
    "                \"gmd:initiativeType/gmd:DS_InitiativeTypeCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "   ]\n",
    "\n",
    "\n",
    "class ISODocument(MappedXmlDocument):\n",
    "\n",
    "    # Attribute specifications from \"XPaths for GEMINI\" by Peter Parslow.\n",
    "    print('in ISODocument %s' % MappedXmlDocument)\n",
    "    elements = [\n",
    "        ISOElement(\n",
    "            name=\"guid\",\n",
    "            search_paths=\"gmd:fileIdentifier/gco:CharacterString/text()\",\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"metadata-language\",\n",
    "            search_paths=[\n",
    "                \"gmd:language/gmd:LanguageCode/@codeListValue\",\n",
    "                \"gmd:language/gmd:LanguageCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"metadata-standard-name\",\n",
    "            search_paths=\"gmd:metadataStandardName/gco:CharacterString/text()\",\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"metadata-standard-version\",\n",
    "            search_paths=\"gmd:metadataStandardVersion/gco:CharacterString/text()\",\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"resource-type\",\n",
    "            search_paths=[\n",
    "                \"gmd:hierarchyLevel/gmd:MD_ScopeCode/@codeListValue\",\n",
    "                \"gmd:hierarchyLevel/gmd:MD_ScopeCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOResponsibleParty(\n",
    "            name=\"metadata-point-of-contact\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:pointOfContact/gmd:CI_ResponsibleParty\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:pointOfContact/gmd:CI_ResponsibleParty\",\n",
    "            ],\n",
    "            multiplicity=\"1..*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"metadata-date\",\n",
    "            search_paths=[\n",
    "                \"gmd:dateStamp/gco:DateTime/text()\",\n",
    "                \"gmd:dateStamp/gco:Date/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"spatial-reference-system\",\n",
    "            search_paths=[\n",
    "                \"gmd:referenceSystemInfo/gmd:MD_ReferenceSystem/gmd:referenceSystemIdentifier/gmd:RS_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"title\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:title/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:title/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"alternate-title\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:alternateTitle/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:alternateTitle/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOReferenceDate(\n",
    "            name=\"dataset-reference-date\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:date/gmd:CI_Date\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:date/gmd:CI_Date\",\n",
    "            ],\n",
    "            multiplicity=\"1..*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"unique-resource-identifier\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:identifier/gmd:MD_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/gmd:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:identifier/gmd:MD_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"presentation-form\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:presentationForm/gmd:CI_PresentationFormCode/text()\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:citation/gmd:CI_Citation/gmd:presentationForm/gmd:CI_PresentationFormCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:presentationForm/gmd:CI_PresentationFormCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:citation/gmd:CI_Citation/gmd:presentationForm/gmd:CI_PresentationFormCode/@codeListValue\",\n",
    "\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"abstract\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:abstract/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:abstract/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"purpose\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:purpose/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:purpose/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOResponsibleParty(\n",
    "            name=\"responsible-organisation\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:pointOfContact/gmd:CI_ResponsibleParty\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:pointOfContact/gmd:CI_ResponsibleParty\",\n",
    "                \"gmd:contact/gmd:CI_ResponsibleParty\",\n",
    "            ],\n",
    "            multiplicity=\"1..*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"frequency-of-update\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceAndUpdateFrequency/gmd:MD_MaintenanceFrequencyCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceAndUpdateFrequency/gmd:MD_MaintenanceFrequencyCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceAndUpdateFrequency/gmd:MD_MaintenanceFrequencyCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceAndUpdateFrequency/gmd:MD_MaintenanceFrequencyCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"maintenance-note\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceNote/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/gmd:SV_ServiceIdentification/gmd:resourceMaintenance/gmd:MD_MaintenanceInformation/gmd:maintenanceNote/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"progress\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:status/gmd:MD_ProgressCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:status/gmd:MD_ProgressCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:status/gmd:MD_ProgressCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:status/gmd:MD_ProgressCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOKeyword(\n",
    "            name=\"keywords\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:descriptiveKeywords/gmd:MD_Keywords\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:descriptiveKeywords/gmd:MD_Keywords\",\n",
    "            ],\n",
    "            multiplicity=\"*\"\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"keyword-inspire-theme\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:descriptiveKeywords/gmd:MD_Keywords/gmd:keyword/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:descriptiveKeywords/gmd:MD_Keywords/gmd:keyword/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        # Deprecated: kept for backwards compatibilty\n",
    "        ISOElement(\n",
    "            name=\"keyword-controlled-other\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:keywords/gmd:MD_Keywords/gmd:keyword/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOUsage(\n",
    "            name=\"usage\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceSpecificUsage/gmd:MD_Usage\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceSpecificUsage/gmd:MD_Usage\",\n",
    "            ],\n",
    "            multiplicity=\"*\"\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"limitations-on-public-access\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:otherConstraints/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:otherConstraints/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"access-constraints\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:accessConstraints/gmd:MD_RestrictionCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:accessConstraints/gmd:MD_RestrictionCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:accessConstraints/gmd:MD_RestrictionCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:accessConstraints/gmd:MD_RestrictionCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "\n",
    "        ISOElement(\n",
    "            name=\"use-constraints\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_Constraints/gmd:useLimitation/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:resourceConstraints/gmd:MD_Constraints/gmd:useLimitation/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOAggregationInfo(\n",
    "            name=\"aggregation-info\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:aggregationInfo/gmd:MD_AggregateInformation\",\n",
    "                \"gmd:identificationInfo/gmd:SV_ServiceIdentification/gmd:aggregationInfo/gmd:MD_AggregateInformation\",\n",
    "            ],\n",
    "            multiplicity=\"*\"\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"spatial-data-service-type\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:serviceType/gco:LocalName/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"spatial-resolution\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:distance/gco:Distance/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:distance/gco:Distance/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"spatial-resolution-units\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:distance/gco:Distance/@uom\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:distance/gco:Distance/@uom\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"equivalent-scale\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:equivalentScale/gmd:MD_RepresentativeFraction/gmd:denominator/gco:Integer/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:spatialResolution/gmd:MD_Resolution/gmd:equivalentScale/gmd:MD_RepresentativeFraction/gmd:denominator/gco:Integer/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"dataset-language\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:language/gmd:LanguageCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:language/gmd:LanguageCode/@codeListValue\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:language/gmd:LanguageCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:language/gmd:LanguageCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"topic-category\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:topicCategory/gmd:MD_TopicCategoryCode/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:topicCategory/gmd:MD_TopicCategoryCode/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"extent-controlled\",\n",
    "            search_paths=[\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"extent-free-text\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:geographicElement/gmd:EX_GeographicDescription/gmd:geographicIdentifier/gmd:MD_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:geographicElement/gmd:EX_GeographicDescription/gmd:geographicIdentifier/gmd:MD_Identifier/gmd:code/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOBoundingBox(\n",
    "            name=\"bbox\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:geographicElement/gmd:EX_GeographicBoundingBox\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:geographicElement/gmd:EX_GeographicBoundingBox\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"temporal-extent-begin\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml:TimePeriod/gml:beginPosition/text()\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml32:TimePeriod/gml32:beginPosition/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml:TimePeriod/gml:beginPosition/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml32:TimePeriod/gml32:beginPosition/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"temporal-extent-end\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml:TimePeriod/gml:endPosition/text()\",\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml32:TimePeriod/gml32:endPosition/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml:TimePeriod/gml:endPosition/text()\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:temporalElement/gmd:EX_TemporalExtent/gmd:extent/gml32:TimePeriod/gml32:endPosition/text()\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"vertical-extent\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:extent/gmd:EX_Extent/gmd:verticalElement/gmd:EX_VerticalExtent\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:extent/gmd:EX_Extent/gmd:verticalElement/gmd:EX_VerticalExtent\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOCoupledResources(\n",
    "            name=\"coupled-resource\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/srv:operatesOn\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"additional-information-source\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:supplementalInformation/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISODataFormat(\n",
    "            name=\"data-format\",\n",
    "            search_paths=[\n",
    "                \"gmd:distributionInfo/gmd:MD_Distribution/gmd:distributionFormat/gmd:MD_Format\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOResponsibleParty(\n",
    "            name=\"distributor\",\n",
    "            search_paths=[\n",
    "                \"gmd:distributionInfo/gmd:MD_Distribution/gmd:distributor/gmd:MD_Distributor/gmd:distributorContact/gmd:CI_ResponsibleParty\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOResourceLocator(\n",
    "            name=\"resource-locator\",\n",
    "            search_paths=[\n",
    "                \"gmd:distributionInfo/gmd:MD_Distribution/gmd:transferOptions/gmd:MD_DigitalTransferOptions/gmd:onLine/gmd:CI_OnlineResource\",\n",
    "                \"gmd:distributionInfo/gmd:MD_Distribution/gmd:distributor/gmd:MD_Distributor/gmd:distributorTransferOptions/gmd:MD_DigitalTransferOptions/gmd:onLine/gmd:CI_OnlineResource\"\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOResourceLocator(\n",
    "            name=\"resource-locator-identification\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo//gmd:CI_OnlineResource\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"conformity-specification\",\n",
    "            search_paths=[\n",
    "                \"gmd:dataQualityInfo/gmd:DQ_DataQuality/gmd:report/gmd:DQ_DomainConsistency/gmd:result/gmd:DQ_ConformanceResult/gmd:specification\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"conformity-pass\",\n",
    "            search_paths=[\n",
    "                \"gmd:dataQualityInfo/gmd:DQ_DataQuality/gmd:report/gmd:DQ_DomainConsistency/gmd:result/gmd:DQ_ConformanceResult/gmd:pass/gco:Boolean/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"conformity-explanation\",\n",
    "            search_paths=[\n",
    "                \"gmd:dataQualityInfo/gmd:DQ_DataQuality/gmd:report/gmd:DQ_DomainConsistency/gmd:result/gmd:DQ_ConformanceResult/gmd:explanation/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOElement(\n",
    "            name=\"lineage\",\n",
    "            search_paths=[\n",
    "                \"gmd:dataQualityInfo/gmd:DQ_DataQuality/gmd:lineage/gmd:LI_Lineage/gmd:statement/gco:CharacterString/text()\",\n",
    "            ],\n",
    "            multiplicity=\"0..1\",\n",
    "        ),\n",
    "        ISOBrowseGraphic(\n",
    "            name=\"browse-graphic\",\n",
    "            search_paths=[\n",
    "                \"gmd:identificationInfo/gmd:MD_DataIdentification/gmd:graphicOverview/gmd:MD_BrowseGraphic\",\n",
    "                \"gmd:identificationInfo/srv:SV_ServiceIdentification/gmd:graphicOverview/gmd:MD_BrowseGraphic\",\n",
    "            ],\n",
    "            multiplicity=\"*\",\n",
    "        ),\n",
    "\n",
    "    ]\n",
    "\n",
    "    def infer_values(self, values):\n",
    "        # Todo: Infer name.\n",
    "        self.infer_date_released(values)\n",
    "        self.infer_date_updated(values)\n",
    "        self.infer_date_created(values)\n",
    "        self.infer_url(values)\n",
    "        # Todo: Infer resources.\n",
    "        self.infer_tags(values)\n",
    "        self.infer_publisher(values)\n",
    "        self.infer_contact(values)\n",
    "        self.infer_contact_email(values)\n",
    "        return values\n",
    "\n",
    "    def infer_date_released(self, values):\n",
    "        value = ''\n",
    "        for date in values['dataset-reference-date']:\n",
    "            if date['type'] == 'publication':\n",
    "                value = date['value']\n",
    "                break\n",
    "        values['date-released'] = value\n",
    "\n",
    "    def infer_date_updated(self, values):\n",
    "        value = ''\n",
    "        dates = []\n",
    "        # Use last of several multiple revision dates.\n",
    "        for date in values['dataset-reference-date']:\n",
    "            if date['type'] == 'revision':\n",
    "                dates.append(date['value'])\n",
    "\n",
    "        if len(dates):\n",
    "            if len(dates) > 1:\n",
    "                dates.sort(reverse=True)\n",
    "            value = dates[0]\n",
    "\n",
    "        values['date-updated'] = value\n",
    "\n",
    "    def infer_date_created(self, values):\n",
    "        value = ''\n",
    "        for date in values['dataset-reference-date']:\n",
    "            if date['type'] == 'creation':\n",
    "                value = date['value']\n",
    "                break\n",
    "        values['date-created'] = value\n",
    "\n",
    "    def infer_url(self, values):\n",
    "        value = ''\n",
    "        for locator in values['resource-locator']:\n",
    "            if locator['function'] == 'information':\n",
    "                value = locator['url']\n",
    "                break\n",
    "        values['url'] = value\n",
    "\n",
    "    def infer_tags(self, values):\n",
    "        tags = []\n",
    "        for key in ['keyword-inspire-theme', 'keyword-controlled-other']:\n",
    "            for item in values[key]:\n",
    "                if item not in tags:\n",
    "                    tags.append(item)\n",
    "        values['tags'] = tags\n",
    "\n",
    "    def infer_publisher(self, values):\n",
    "        value = ''\n",
    "        for responsible_party in values['responsible-organisation']:\n",
    "            if responsible_party['role'] == 'publisher':\n",
    "                value = responsible_party['organisation-name']\n",
    "            if value:\n",
    "                break\n",
    "        values['publisher'] = value\n",
    "\n",
    "    def infer_contact(self, values):\n",
    "        value = ''\n",
    "        for responsible_party in values['responsible-organisation']:\n",
    "            value = responsible_party['organisation-name']\n",
    "            if value:\n",
    "                break\n",
    "        values['contact'] = value\n",
    "\n",
    "    def infer_contact_email(self, values):\n",
    "        value = ''\n",
    "        for responsible_party in values['responsible-organisation']:\n",
    "            if isinstance(responsible_party, dict) and \\\n",
    "               isinstance(responsible_party.get('contact-info'), dict) and \\\n",
    "               responsible_party['contact-info'].has_key('email'):\n",
    "                value = responsible_party['contact-info']['email']\n",
    "                if value:\n",
    "                    break\n",
    "        values['contact-email'] = value\n",
    "\n",
    "\n",
    "class GeminiDocument(ISODocument):\n",
    "    '''\n",
    "    For backwards compatibility\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Harvesters base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load E:\\GitHub\\ckan\\ckanext-spatial\\ckanext\\spatial\\harvesters\\base.py\n",
    "\n",
    "#from ckanext.harvest.harvesters.base import munge_tag\n",
    "\n",
    "#from ckanext.harvest.harvesters.base import HarvesterBase\n",
    "#from ckanext.harvest.model import HarvestObject\n",
    "\n",
    "#from ckanext.spatial.validation import Validators, all_validators\n",
    "#from ckanext.spatial.model import ISODocument\n",
    "#from ckanext.spatial.interfaces import ISpatialHarvester\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_VALIDATOR_PROFILES = ['iso19139']\n",
    "\n",
    "\n",
    "def text_traceback():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        res = 'the original traceback:'.join(\n",
    "            cgitb.text(sys.exc_info()).split('the original traceback:')[1:]\n",
    "        ).strip()\n",
    "    return res\n",
    "\n",
    "\n",
    "def guess_standard(content):\n",
    "    lowered = content.lower()\n",
    "    if '</gmd:MD_Metadata>'.lower() in lowered:\n",
    "        return 'iso'\n",
    "    if '</gmi:MI_Metadata>'.lower() in lowered:\n",
    "        return 'iso'\n",
    "    if '</metadata>'.lower() in lowered:\n",
    "        return 'fgdc'\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def guess_resource_format(url, use_mimetypes=True):\n",
    "    '''\n",
    "    Given a URL try to guess the best format to assign to the resource\n",
    "\n",
    "    The function looks for common patterns in popular geospatial services and\n",
    "    file extensions, so it may not be 100% accurate. It just looks at the\n",
    "    provided URL, it does not attempt to perform any remote check.\n",
    "\n",
    "    if 'use_mimetypes' is True (default value), the mimetypes module will be\n",
    "    used if no match was found before.\n",
    "\n",
    "    Returns None if no format could be guessed.\n",
    "\n",
    "    '''\n",
    "    url = url.lower().strip()\n",
    "\n",
    "    resource_types = {\n",
    "        # OGC\n",
    "        'wms': ('service=wms', 'geoserver/wms', 'mapserver/wmsserver', 'com.esri.wms.Esrimap', 'service/wms'),\n",
    "        'wfs': ('service=wfs', 'geoserver/wfs', 'mapserver/wfsserver', 'com.esri.wfs.Esrimap'),\n",
    "        'wcs': ('service=wcs', 'geoserver/wcs', 'imageserver/wcsserver', 'mapserver/wcsserver'),\n",
    "        'sos': ('service=sos',),\n",
    "        'csw': ('service=csw',),\n",
    "        # ESRI\n",
    "        'kml': ('mapserver/generatekml',),\n",
    "        'arcims': ('com.esri.esrimap.esrimap',),\n",
    "        'arcgis_rest': ('arcgis/rest/services',),\n",
    "    }\n",
    "\n",
    "    for resource_type, parts in resource_types.iteritems():\n",
    "        if any(part in url for part in parts):\n",
    "            return resource_type\n",
    "\n",
    "    file_types = {\n",
    "        'kml' : ('kml',),\n",
    "        'kmz': ('kmz',),\n",
    "        'gml': ('gml',),\n",
    "    }\n",
    "\n",
    "    for file_type, extensions in file_types.iteritems():\n",
    "        if any(url.endswith(extension) for extension in extensions):\n",
    "            return file_type\n",
    "\n",
    "    resource_format, encoding = mimetypes.guess_type(url)\n",
    "    if resource_format:\n",
    "        return resource_format\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "class SpatialHarvester(HarvesterBase):\n",
    "\n",
    "    _user_name = None\n",
    "\n",
    "    _site_user = None\n",
    "\n",
    "    source_config = {}\n",
    "\n",
    "    force_import = False\n",
    "\n",
    "    extent_template = Template('''\n",
    "    {\"type\": \"Polygon\", \"coordinates\": [[[$xmin, $ymin], [$xmax, $ymin], [$xmax, $ymax], [$xmin, $ymax], [$xmin, $ymin]]]}\n",
    "    ''')\n",
    "\n",
    "    ## IHarvester\n",
    "\n",
    "    def validate_config(self, source_config):\n",
    "        if not source_config:\n",
    "            return source_config\n",
    "\n",
    "        try:\n",
    "            source_config_obj = json.loads(source_config)\n",
    "\n",
    "            if 'validator_profiles' in source_config_obj:\n",
    "                if not isinstance(source_config_obj['validator_profiles'], list):\n",
    "                    raise ValueError('validator_profiles must be a list')\n",
    "\n",
    "                # Check if all profiles exist\n",
    "                existing_profiles = [v.name for v in all_validators]\n",
    "                unknown_profiles = set(source_config_obj['validator_profiles']) - set(existing_profiles)\n",
    "\n",
    "                if len(unknown_profiles) > 0:\n",
    "                    raise ValueError('Unknown validation profile(s): %s' % ','.join(unknown_profiles))\n",
    "\n",
    "            if 'default_tags' in source_config_obj:\n",
    "                if not isinstance(source_config_obj['default_tags'],list):\n",
    "                    raise ValueError('default_tags must be a list')\n",
    "\n",
    "            if 'default_extras' in source_config_obj:\n",
    "                if not isinstance(source_config_obj['default_extras'],dict):\n",
    "                    raise ValueError('default_extras must be a dictionary')\n",
    "\n",
    "            for key in ('override_extras', 'clean_tags'):\n",
    "                if key in source_config_obj:\n",
    "                    if not isinstance(source_config_obj[key],bool):\n",
    "                        raise ValueError('%s must be boolean' % key)\n",
    "\n",
    "        except ValueError, e:\n",
    "            raise e\n",
    "\n",
    "        return source_config\n",
    "\n",
    "    ##\n",
    "\n",
    "    ## SpatialHarvester\n",
    "\n",
    "\n",
    "    def get_package_dict(self, iso_values, harvest_object):\n",
    "        '''\n",
    "        Constructs a package_dict suitable to be passed to package_create or\n",
    "        package_update. See documentation on\n",
    "        ckan.logic.action.create.package_create for more details\n",
    "\n",
    "        Extensions willing to modify the dict should do so implementing the\n",
    "        ISpatialHarvester interface\n",
    "\n",
    "            import ckan.plugins as p\n",
    "            from ckanext.spatial.interfaces import ISpatialHarvester\n",
    "\n",
    "            class MyHarvester(p.SingletonPlugin):\n",
    "\n",
    "                p.implements(ISpatialHarvester, inherit=True)\n",
    "\n",
    "                def get_package_dict(self, context, data_dict):\n",
    "\n",
    "                    package_dict = data_dict['package_dict']\n",
    "\n",
    "                    package_dict['extras'].append(\n",
    "                        {'key': 'my-custom-extra', 'value': 'my-custom-value'}\n",
    "                    )\n",
    "\n",
    "                    return package_dict\n",
    "\n",
    "        If a dict is not returned by this function, the import stage will be cancelled.\n",
    "\n",
    "        :param iso_values: Dictionary with parsed values from the ISO 19139\n",
    "            XML document\n",
    "        :type iso_values: dict\n",
    "        :param harvest_object: HarvestObject domain object (with access to\n",
    "            job and source objects)\n",
    "        :type harvest_object: HarvestObject\n",
    "\n",
    "        :returns: A dataset dictionary (package_dict)\n",
    "        :rtype: dict\n",
    "        '''\n",
    "        \n",
    "        tags = []\n",
    "\n",
    "        if 'tags' in iso_values:\n",
    "            do_clean = self.source_config.get('clean_tags')\n",
    "            tags_val = [munge_tag(tag) if do_clean else tag[:100] for tag in iso_values['tags']]\n",
    "            tags = [{'name': tag} for tag in tags_val]\n",
    "\n",
    "        # Add default_tags from config\n",
    "        default_tags = self.source_config.get('default_tags', [])\n",
    "        if default_tags:\n",
    "            for tag in default_tags:\n",
    "                tags.append({'name': tag})\n",
    "\n",
    "        package_dict = {\n",
    "            'title': iso_values['title'],\n",
    "            'notes': iso_values['abstract'],\n",
    "            'tags': tags,\n",
    "            'resources': [],\n",
    "        }\n",
    "\n",
    "        # We need to get the owner organization (if any) from the harvest\n",
    "        # source dataset\n",
    "        source_dataset = model.Package.get(harvest_object.source.id)\n",
    "        if source_dataset.owner_org:\n",
    "            package_dict['owner_org'] = source_dataset.owner_org\n",
    "\n",
    "        # Package name\n",
    "        package = harvest_object.package\n",
    "        if package is None or package.title != iso_values['title']:\n",
    "            name = self._gen_new_name(iso_values['title'])\n",
    "            if not name:\n",
    "                name = self._gen_new_name(str(iso_values['guid']))\n",
    "            if not name:\n",
    "                raise Exception('Could not generate a unique name from the title or the GUID. Please choose a more unique title.')\n",
    "            package_dict['name'] = name\n",
    "        else:\n",
    "            package_dict['name'] = package.name\n",
    "\n",
    "        extras = {\n",
    "            'guid': harvest_object.guid,\n",
    "            'spatial_harvester': True,\n",
    "        }\n",
    "\n",
    "        # Just add some of the metadata as extras, not the whole lot\n",
    "        for name in [\n",
    "            # Essentials\n",
    "            'spatial-reference-system',\n",
    "            'guid',\n",
    "            # Usefuls\n",
    "            'dataset-reference-date',\n",
    "            'metadata-language',  # Language\n",
    "            'metadata-date',  # Released\n",
    "            'coupled-resource',\n",
    "            'contact-email',\n",
    "            'frequency-of-update',\n",
    "            'spatial-data-service-type',\n",
    "        ]:\n",
    "            extras[name] = iso_values[name]\n",
    "\n",
    "        if len(iso_values.get('progress', [])):\n",
    "            extras['progress'] = iso_values['progress'][0]\n",
    "        else:\n",
    "            extras['progress'] = ''\n",
    "\n",
    "        if len(iso_values.get('resource-type', [])):\n",
    "            extras['resource-type'] = iso_values['resource-type'][0]\n",
    "        else:\n",
    "            extras['resource-type'] = ''\n",
    "\n",
    "        extras['licence'] = iso_values.get('use-constraints', '')\n",
    "\n",
    "        def _extract_first_license_url(licences):\n",
    "            for licence in licences:\n",
    "                o = urlparse(licence)\n",
    "                if o.scheme and o.netloc:\n",
    "                    return licence\n",
    "            return None\n",
    "\n",
    "        if len(extras['licence']):\n",
    "            license_url_extracted = _extract_first_license_url(extras['licence'])\n",
    "            if license_url_extracted:\n",
    "                extras['licence_url'] = license_url_extracted\n",
    "\n",
    "\n",
    "        # Metadata license ID check for package\n",
    "        use_constraints = iso_values.get('use-constraints')\n",
    "        if use_constraints:\n",
    "\n",
    "            context = {'model': model, 'session': model.Session, 'user': self._get_user_name()}\n",
    "            license_list = p.toolkit.get_action('license_list')(context, {})\n",
    "\n",
    "            for constraint in use_constraints:\n",
    "                package_license = None\n",
    "\n",
    "                for license in license_list:\n",
    "                    if constraint.lower() == license.get('id') or constraint == license.get('url'):\n",
    "                        package_license = license.get('id')\n",
    "                        break\n",
    "\n",
    "                if package_license:\n",
    "                    package_dict['license_id'] = package_license\n",
    "                    break\n",
    "\n",
    "\n",
    "        extras['access_constraints'] = iso_values.get('limitations-on-public-access', '')\n",
    "\n",
    "        # Grpahic preview\n",
    "        browse_graphic = iso_values.get('browse-graphic')\n",
    "        if browse_graphic:\n",
    "            browse_graphic = browse_graphic[0]\n",
    "            extras['graphic-preview-file'] = browse_graphic.get('file')\n",
    "            if browse_graphic.get('description'):\n",
    "                extras['graphic-preview-description'] = browse_graphic.get('description')\n",
    "            if browse_graphic.get('type'):\n",
    "                extras['graphic-preview-type'] = browse_graphic.get('type')\n",
    "\n",
    "\n",
    "        for key in ['temporal-extent-begin', 'temporal-extent-end']:\n",
    "            if len(iso_values[key]) > 0:\n",
    "                extras[key] = iso_values[key][0]\n",
    "\n",
    "        # Save responsible organization roles\n",
    "        if iso_values['responsible-organisation']:\n",
    "            parties = {}\n",
    "            for party in iso_values['responsible-organisation']:\n",
    "                if party['organisation-name'] in parties:\n",
    "                    if not party['role'] in parties[party['organisation-name']]:\n",
    "                        parties[party['organisation-name']].append(party['role'])\n",
    "                else:\n",
    "                    parties[party['organisation-name']] = [party['role']]\n",
    "            extras['responsible-party'] = [{'name': k, 'roles': v} for k, v in parties.iteritems()]\n",
    "\n",
    "        if len(iso_values['bbox']) > 0:\n",
    "            bbox = iso_values['bbox'][0]\n",
    "            extras['bbox-east-long'] = bbox['east']\n",
    "            extras['bbox-north-lat'] = bbox['north']\n",
    "            extras['bbox-south-lat'] = bbox['south']\n",
    "            extras['bbox-west-long'] = bbox['west']\n",
    "\n",
    "            try:\n",
    "                xmin = float(bbox['west'])\n",
    "                xmax = float(bbox['east'])\n",
    "                ymin = float(bbox['south'])\n",
    "                ymax = float(bbox['north'])\n",
    "            except ValueError, e:\n",
    "                self._save_object_error('Error parsing bounding box value: {0}'.format(str(e)),\n",
    "                                    harvest_object, 'Import')\n",
    "            else:\n",
    "                # Construct a GeoJSON extent so ckanext-spatial can register the extent geometry\n",
    "\n",
    "                # Some publishers define the same two corners for the bbox (ie a point),\n",
    "                # that causes problems in the search if stored as polygon\n",
    "                if xmin == xmax or ymin == ymax:\n",
    "                    extent_string = Template('{\"type\": \"Point\", \"coordinates\": [$x, $y]}').substitute(\n",
    "                        x=xmin, y=ymin\n",
    "                    )\n",
    "                    self._save_object_error('Point extent defined instead of polygon',\n",
    "                                     harvest_object, 'Import')\n",
    "                else:\n",
    "                    extent_string = self.extent_template.substitute(\n",
    "                        xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax\n",
    "                    )\n",
    "\n",
    "                extras['spatial'] = extent_string.strip()\n",
    "        else:\n",
    "            log.debug('No spatial extent defined for this object')\n",
    "\n",
    "        resource_locators = iso_values.get('resource-locator', []) +\\\n",
    "            iso_values.get('resource-locator-identification', [])\n",
    "\n",
    "        if len(resource_locators):\n",
    "            for resource_locator in resource_locators:\n",
    "                url = resource_locator.get('url', '').strip()\n",
    "                if url:\n",
    "                    resource = {}\n",
    "                    resource['format'] = guess_resource_format(url)\n",
    "                    if resource['format'] == 'wms' and config.get('ckanext.spatial.harvest.validate_wms', False):\n",
    "                        # Check if the service is a view service\n",
    "                        test_url = url.split('?')[0] if '?' in url else url\n",
    "                        if self._is_wms(test_url):\n",
    "                            resource['verified'] = True\n",
    "                            resource['verified_date'] = datetime.now().isoformat()\n",
    "\n",
    "                    resource.update(\n",
    "                        {\n",
    "                            'url': url,\n",
    "                            'name': resource_locator.get('name') or p.toolkit._('Unnamed resource'),\n",
    "                            'description': resource_locator.get('description') or  '',\n",
    "                            'resource_locator_protocol': resource_locator.get('protocol') or '',\n",
    "                            'resource_locator_function': resource_locator.get('function') or '',\n",
    "                        })\n",
    "                    package_dict['resources'].append(resource)\n",
    "\n",
    "\n",
    "        # Add default_extras from config\n",
    "        default_extras = self.source_config.get('default_extras',{})\n",
    "        if default_extras:\n",
    "           override_extras = self.source_config.get('override_extras',False)\n",
    "           for key,value in default_extras.iteritems():\n",
    "              log.debug('Processing extra %s', key)\n",
    "              if not key in extras or override_extras:\n",
    "                 # Look for replacement strings\n",
    "                 if isinstance(value,basestring):\n",
    "                    value = value.format(harvest_source_id=harvest_object.job.source.id,\n",
    "                             harvest_source_url=harvest_object.job.source.url.strip('/'),\n",
    "                             harvest_source_title=harvest_object.job.source.title,\n",
    "                             harvest_job_id=harvest_object.job.id,\n",
    "                             harvest_object_id=harvest_object.id)\n",
    "                 extras[key] = value\n",
    "\n",
    "        extras_as_dict = []\n",
    "        for key, value in extras.iteritems():\n",
    "            if isinstance(value, (list, dict)):\n",
    "                extras_as_dict.append({'key': key, 'value': json.dumps(value)})\n",
    "            else:\n",
    "                extras_as_dict.append({'key': key, 'value': value})\n",
    "\n",
    "        package_dict['extras'] = extras_as_dict\n",
    "\n",
    "        return package_dict\n",
    "\n",
    "    def transform_to_iso(self, original_document, original_format, harvest_object):\n",
    "        '''\n",
    "        DEPRECATED: Use the transform_to_iso method of the ISpatialHarvester\n",
    "        interface\n",
    "        '''\n",
    "        self.__base_transform_to_iso_called = True\n",
    "        return None\n",
    "\n",
    "    def import_stage(self, harvest_object):\n",
    "        context = {\n",
    "            'model': model,\n",
    "            'session': model.Session,\n",
    "            'user': self._get_user_name(),\n",
    "        }\n",
    "\n",
    "        log = logging.getLogger(__name__ + '.import')\n",
    "        log.debug('Import stage for harvest object: %s', harvest_object.id)\n",
    "\n",
    "        if not harvest_object:\n",
    "            log.error('No harvest object received')\n",
    "            return False\n",
    "\n",
    "        self._set_source_config(harvest_object.source.config)\n",
    "\n",
    "        if self.force_import:\n",
    "            status = 'change'\n",
    "        else:\n",
    "            status = self._get_object_extra(harvest_object, 'status')\n",
    "\n",
    "        # Get the last harvested object (if any)\n",
    "        previous_object = model.Session.query(HarvestObject) \\\n",
    "                          .filter(HarvestObject.guid==harvest_object.guid) \\\n",
    "                          .filter(HarvestObject.current==True) \\\n",
    "                          .first()\n",
    "\n",
    "        if status == 'delete':\n",
    "            # Delete package\n",
    "            context.update({\n",
    "                'ignore_auth': True,\n",
    "            })\n",
    "            p.toolkit.get_action('package_delete')(context, {'id': harvest_object.package_id})\n",
    "            log.info('Deleted package {0} with guid {1}'.format(harvest_object.package_id, harvest_object.guid))\n",
    "\n",
    "            return True\n",
    "\n",
    "        # Check if it is a non ISO document\n",
    "        original_document = self._get_object_extra(harvest_object, 'original_document')\n",
    "        original_format = self._get_object_extra(harvest_object, 'original_format')\n",
    "        if original_document and original_format:\n",
    "            #DEPRECATED use the ISpatialHarvester interface method\n",
    "            self.__base_transform_to_iso_called = False\n",
    "            content = self.transform_to_iso(original_document, original_format, harvest_object)\n",
    "            if not self.__base_transform_to_iso_called:\n",
    "                log.warn('Deprecation warning: calling transform_to_iso directly is deprecated. ' +\n",
    "                         'Please use the ISpatialHarvester interface method instead.')\n",
    "\n",
    "            for harvester in p.PluginImplementations(ISpatialHarvester):\n",
    "                content = harvester.transform_to_iso(original_document, original_format, harvest_object)\n",
    "\n",
    "            if content:\n",
    "                harvest_object.content = content\n",
    "            else:\n",
    "                self._save_object_error('Transformation to ISO failed', harvest_object, 'Import')\n",
    "                return False\n",
    "        else:\n",
    "            if harvest_object.content is None:\n",
    "                self._save_object_error('Empty content for object {0}'.format(harvest_object.id), harvest_object, 'Import')\n",
    "                return False\n",
    "\n",
    "            # Validate ISO document\n",
    "            is_valid, profile, errors = self._validate_document(harvest_object.content, harvest_object)\n",
    "            if not is_valid:\n",
    "                # If validation errors were found, import will stop unless\n",
    "                # configuration per source or per instance says otherwise\n",
    "                continue_import = p.toolkit.asbool(config.get('ckanext.spatial.harvest.continue_on_validation_errors', False)) or \\\n",
    "                    self.source_config.get('continue_on_validation_errors')\n",
    "                if not continue_import:\n",
    "                    return False\n",
    "\n",
    "        # Parse ISO document\n",
    "        try:\n",
    "\n",
    "            iso_parser = ISODocument(harvest_object.content)\n",
    "            iso_values = iso_parser.read_values()\n",
    "        except Exception, e:\n",
    "            self._save_object_error('Error parsing ISO document for object {0}: {1}'.format(harvest_object.id, str(e)),\n",
    "                                    harvest_object, 'Import')\n",
    "            return False\n",
    "\n",
    "        # Flag previous object as not current anymore\n",
    "        if previous_object and not self.force_import:\n",
    "            previous_object.current = False\n",
    "            previous_object.add()\n",
    "\n",
    "        # Update GUID with the one on the document\n",
    "        iso_guid = iso_values['guid']\n",
    "        if iso_guid and harvest_object.guid != iso_guid:\n",
    "            # First make sure there already aren't current objects\n",
    "            # with the same guid\n",
    "            existing_object = model.Session.query(HarvestObject.id) \\\n",
    "                            .filter(HarvestObject.guid==iso_guid) \\\n",
    "                            .filter(HarvestObject.current==True) \\\n",
    "                            .first()\n",
    "            if existing_object:\n",
    "                self._save_object_error('Object {0} already has this guid {1}'.format(existing_object.id, iso_guid),\n",
    "                        harvest_object, 'Import')\n",
    "                return False\n",
    "\n",
    "            harvest_object.guid = iso_guid\n",
    "            harvest_object.add()\n",
    "\n",
    "        # Generate GUID if not present (i.e. it's a manual import)\n",
    "        if not harvest_object.guid:\n",
    "            m = hashlib.md5()\n",
    "            m.update(harvest_object.content.encode('utf8', 'ignore'))\n",
    "            harvest_object.guid = m.hexdigest()\n",
    "            harvest_object.add()\n",
    "\n",
    "        # Get document modified date\n",
    "        try:\n",
    "            metadata_modified_date = dateutil.parser.parse(iso_values['metadata-date'], ignoretz=True)\n",
    "        except ValueError:\n",
    "            self._save_object_error('Could not extract reference date for object {0} ({1})'\n",
    "                        .format(harvest_object.id, iso_values['metadata-date']), harvest_object, 'Import')\n",
    "            return False\n",
    "\n",
    "        harvest_object.metadata_modified_date = metadata_modified_date\n",
    "        harvest_object.add()\n",
    "\n",
    "\n",
    "        # Build the package dict\n",
    "        package_dict = self.get_package_dict(iso_values, harvest_object)\n",
    "        for harvester in p.PluginImplementations(ISpatialHarvester):\n",
    "            package_dict = harvester.get_package_dict(context, {\n",
    "                'package_dict': package_dict,\n",
    "                'iso_values': iso_values,\n",
    "                'xml_tree': iso_parser.xml_tree,\n",
    "                'harvest_object': harvest_object,\n",
    "            })\n",
    "        if not package_dict:\n",
    "            log.error('No package dict returned, aborting import for object {0}'.format(harvest_object.id))\n",
    "            return False\n",
    "\n",
    "        # Create / update the package\n",
    "        context.update({\n",
    "           'extras_as_string': True,\n",
    "           'api_version': '2',\n",
    "           'return_id_only': True})\n",
    "\n",
    "        if self._site_user and context['user'] == self._site_user['name']:\n",
    "            context['ignore_auth'] = True\n",
    "\n",
    "\n",
    "        # The default package schema does not like Upper case tags\n",
    "        tag_schema = logic.schema.default_tags_schema()\n",
    "        tag_schema['name'] = [not_empty, unicode]\n",
    "\n",
    "        # Flag this object as the current one\n",
    "        harvest_object.current = True\n",
    "        harvest_object.add()\n",
    "\n",
    "        if status == 'new':\n",
    "            package_schema = logic.schema.default_create_package_schema()\n",
    "            package_schema['tags'] = tag_schema\n",
    "            context['schema'] = package_schema\n",
    "\n",
    "            # We need to explicitly provide a package ID, otherwise ckanext-spatial\n",
    "            # won't be be able to link the extent to the package.\n",
    "            package_dict['id'] = unicode(uuid.uuid4())\n",
    "            package_schema['id'] = [unicode]\n",
    "\n",
    "            # Save reference to the package on the object\n",
    "            harvest_object.package_id = package_dict['id']\n",
    "            harvest_object.add()\n",
    "            # Defer constraints and flush so the dataset can be indexed with\n",
    "            # the harvest object id (on the after_show hook from the harvester\n",
    "            # plugin)\n",
    "            model.Session.execute('SET CONSTRAINTS harvest_object_package_id_fkey DEFERRED')\n",
    "            model.Session.flush()\n",
    "\n",
    "            try:\n",
    "                package_id = p.toolkit.get_action('package_create')(context, package_dict)\n",
    "                log.info('Created new package %s with guid %s', package_id, harvest_object.guid)\n",
    "            except p.toolkit.ValidationError, e:\n",
    "                self._save_object_error('Validation Error: %s' % str(e.error_summary), harvest_object, 'Import')\n",
    "                return False\n",
    "\n",
    "        elif status == 'change':\n",
    "\n",
    "            # Check if the modified date is more recent\n",
    "            if not self.force_import and previous_object and harvest_object.metadata_modified_date <= previous_object.metadata_modified_date:\n",
    "\n",
    "                # Assign the previous job id to the new object to\n",
    "                # avoid losing history\n",
    "                harvest_object.harvest_job_id = previous_object.job.id\n",
    "                harvest_object.add()\n",
    "\n",
    "                # Delete the previous object to avoid cluttering the object table\n",
    "                previous_object.delete()\n",
    "\n",
    "                # Reindex the corresponding package to update the reference to the\n",
    "                # harvest object\n",
    "                if ((config.get('ckanext.spatial.harvest.reindex_unchanged', True) != 'False'\n",
    "                    or self.source_config.get('reindex_unchanged') != 'False')\n",
    "                    and harvest_object.package_id):\n",
    "                    context.update({'validate': False, 'ignore_auth': True})\n",
    "                    try:\n",
    "                        package_dict = logic.get_action('package_show')(context,\n",
    "                            {'id': harvest_object.package_id})\n",
    "                    except p.toolkit.ObjectNotFound:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for extra in package_dict.get('extras', []):\n",
    "                            if extra['key'] == 'harvest_object_id':\n",
    "                                extra['value'] = harvest_object.id\n",
    "                        if package_dict:\n",
    "                            package_index = PackageSearchIndex()\n",
    "                            package_index.index_package(package_dict)\n",
    "\n",
    "                log.info('Document with GUID %s unchanged, skipping...' % (harvest_object.guid))\n",
    "            else:\n",
    "                package_schema = logic.schema.default_update_package_schema()\n",
    "                package_schema['tags'] = tag_schema\n",
    "                context['schema'] = package_schema\n",
    "\n",
    "                package_dict['id'] = harvest_object.package_id\n",
    "                try:\n",
    "                    package_id = p.toolkit.get_action('package_update')(context, package_dict)\n",
    "                    log.info('Updated package %s with guid %s', package_id, harvest_object.guid)\n",
    "                except p.toolkit.ValidationError, e:\n",
    "                    self._save_object_error('Validation Error: %s' % str(e.error_summary), harvest_object, 'Import')\n",
    "                    return False\n",
    "\n",
    "        model.Session.commit()\n",
    "\n",
    "        return True\n",
    "    ##\n",
    "\n",
    "    def _is_wms(self, url):\n",
    "        '''\n",
    "        Checks if the provided URL actually points to a Web Map Service.\n",
    "        Uses owslib WMS reader to parse the response.\n",
    "        '''\n",
    "        try:\n",
    "            capabilities_url = wms.WMSCapabilitiesReader().capabilities_url(url)\n",
    "            res = urllib2.urlopen(capabilities_url, None, 10)\n",
    "            xml = res.read()\n",
    "\n",
    "            s = wms.WebMapService(url, xml=xml)\n",
    "            return isinstance(s.contents, dict) and s.contents != {}\n",
    "        except Exception, e:\n",
    "            log.error('WMS check for %s failed with exception: %s' % (url, str(e)))\n",
    "        return False\n",
    "\n",
    "    def _get_object_extra(self, harvest_object, key):\n",
    "        '''\n",
    "        Helper function for retrieving the value from a harvest object extra,\n",
    "        given the key\n",
    "        '''\n",
    "        for extra in harvest_object.extras:\n",
    "            if extra.key == key:\n",
    "                return extra.value\n",
    "        return None\n",
    "\n",
    "    def _set_source_config(self, config_str):\n",
    "        '''\n",
    "        Loads the source configuration JSON object into a dict for\n",
    "        convenient access\n",
    "        '''\n",
    "        if config_str:\n",
    "            self.source_config = json.loads(config_str)\n",
    "            log.debug('Using config: %r', self.source_config)\n",
    "        else:\n",
    "            self.source_config = {}\n",
    "\n",
    "    def _get_validator(self):\n",
    "        '''\n",
    "        Returns the validator object using the relevant profiles\n",
    "\n",
    "        The profiles to be used are assigned in the following order:\n",
    "\n",
    "        1. 'validator_profiles' property of the harvest source config object\n",
    "        2. 'ckan.spatial.validator.profiles' configuration option in the ini file\n",
    "        3. Default value as defined in DEFAULT_VALIDATOR_PROFILES\n",
    "        '''\n",
    "        if not hasattr(self, '_validator'):\n",
    "            if hasattr(self, 'source_config') and self.source_config.get('validator_profiles', None):\n",
    "                profiles = self.source_config.get('validator_profiles')\n",
    "            elif config.get('ckan.spatial.validator.profiles', None):\n",
    "                profiles = [\n",
    "                    x.strip() for x in\n",
    "                    config.get('ckan.spatial.validator.profiles').split(',')\n",
    "                ]\n",
    "            else:\n",
    "                profiles = DEFAULT_VALIDATOR_PROFILES\n",
    "            self._validator = Validators(profiles=profiles)\n",
    "\n",
    "            # Add any custom validators from extensions\n",
    "            for plugin_with_validators in p.PluginImplementations(ISpatialHarvester):\n",
    "                custom_validators = plugin_with_validators.get_validators()\n",
    "                for custom_validator in custom_validators:\n",
    "                    if custom_validator not in all_validators:\n",
    "                        self._validator.add_validator(custom_validator)\n",
    "\n",
    "\n",
    "        return self._validator\n",
    "\n",
    "    def _get_user_name(self):\n",
    "        '''\n",
    "        Returns the name of the user that will perform the harvesting actions\n",
    "        (deleting, updating and creating datasets)\n",
    "\n",
    "        By default this will be the internal site admin user. This is the\n",
    "        recommended setting, but if necessary it can be overridden with the\n",
    "        `ckanext.spatial.harvest.user_name` config option, eg to support the\n",
    "        old hardcoded 'harvest' user:\n",
    "\n",
    "           ckanext.spatial.harvest.user_name = harvest\n",
    "\n",
    "        '''\n",
    "        if self._user_name:\n",
    "            return self._user_name\n",
    "\n",
    "        context = {'model': model,\n",
    "                   'ignore_auth': True,\n",
    "                   'defer_commit': True, # See ckan/ckan#1714\n",
    "                  }\n",
    "        self._site_user = p.toolkit.get_action('get_site_user')(context, {})\n",
    "\n",
    "        config_user_name = config.get('ckanext.spatial.harvest.user_name')\n",
    "        if config_user_name:\n",
    "            self._user_name = config_user_name\n",
    "        else:\n",
    "            self._user_name = self._site_user['name']\n",
    "\n",
    "        return self._user_name\n",
    "\n",
    "    def _get_content(self, url):\n",
    "        '''\n",
    "        DEPRECATED: Use _get_content_as_unicode instead\n",
    "        '''\n",
    "        url = url.replace(' ', '%20')\n",
    "        http_response = urllib2.urlopen(url)\n",
    "        return http_response.read()\n",
    "\n",
    "    def _get_content_as_unicode(self, url):\n",
    "        '''\n",
    "        Get remote content as unicode.\n",
    "\n",
    "        We let requests handle the conversion [1] , which will use the\n",
    "        content-type header first or chardet if the header is missing\n",
    "        (requests uses its own embedded chardet version).\n",
    "\n",
    "        As we will be storing and serving the contents as unicode, we actually\n",
    "        replace the original XML encoding declaration with an UTF-8 one.\n",
    "\n",
    "\n",
    "        [1] http://github.com/kennethreitz/requests/blob/63243b1e3b435c7736acf1e51c0f6fa6666d861d/requests/models.py#L811\n",
    "\n",
    "        '''\n",
    "        url = url.replace(' ', '%20')\n",
    "        response = requests.get(url, timeout=10)\n",
    "\n",
    "        content = response.text\n",
    "\n",
    "        # Remove original XML declaration\n",
    "        content = re.sub('<\\?xml(.*)\\?>', '', content)\n",
    "\n",
    "        # Get rid of the BOM and other rubbish at the beginning of the file\n",
    "        content = re.sub('.*?<', '<', content, 1)\n",
    "        content = content[content.index('<'):]\n",
    "\n",
    "        return content\n",
    "\n",
    "    def _validate_document(self, document_string, harvest_object, validator=None):\n",
    "        '''\n",
    "        Validates an XML document with the default, or if present, the\n",
    "        provided validators.\n",
    "\n",
    "        It will create a HarvestObjectError for each validation error found,\n",
    "        so they can be shown properly on the frontend.\n",
    "\n",
    "        Returns a tuple, with a boolean showing whether the validation passed\n",
    "        or not, the profile used and a list of errors (tuples with error\n",
    "        message and error lines if present).\n",
    "        '''\n",
    "        if not validator:\n",
    "            validator = self._get_validator()\n",
    "\n",
    "        document_string = re.sub('<\\?xml(.*)\\?>', '', document_string)\n",
    "\n",
    "        try:\n",
    "            xml = etree.fromstring(document_string)\n",
    "        except etree.XMLSyntaxError, e:\n",
    "            self._save_object_error('Could not parse XML file: {0}'.format(str(e)), harvest_object, 'Import')\n",
    "            return False, None, []\n",
    "\n",
    "        valid, profile, errors = validator.is_valid(xml)\n",
    "        if not valid:\n",
    "            log.error('Validation errors found using profile {0} for object with GUID {1}'.format(profile, harvest_object.guid))\n",
    "            for error in errors:\n",
    "                self._save_object_error(error[0], harvest_object, 'Validation', line=error[1])\n",
    "\n",
    "        return valid, profile, errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesters CSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load E:\\GitHub\\ckan\\ckanext-spatial\\ckanext\\spatial\\harvesters\\csw.py\n",
    "\n",
    "#from ckanext.harvest.interfaces import IHarvester\n",
    "#from ckanext.harvest.model import HarvestObject\n",
    "#from ckanext.harvest.model import HarvestObjectExtra as HOExtra\n",
    "\n",
    "#from ckanext.spatial.lib.csw_client import CswService\n",
    "#from ckanext.spatial.harvesters.base import SpatialHarvester, text_traceback\n",
    "\n",
    "\n",
    "class CSWHarvester(SpatialHarvester, SingletonPlugin):\n",
    "    '''\n",
    "    A Harvester for CSW servers\n",
    "    '''\n",
    "    implements(IHarvester)\n",
    "\n",
    "    csw=None\n",
    "\n",
    "    def info(self):\n",
    "        return {\n",
    "            'name': 'csw',\n",
    "            'title': 'CSW Server',\n",
    "            'description': 'A server that implements OGC\\'s Catalog Service for the Web (CSW) standard'\n",
    "            }\n",
    "\n",
    "\n",
    "    def get_original_url(self, harvest_object_id):\n",
    "        obj = model.Session.query(HarvestObject).\\\n",
    "                                    filter(HarvestObject.id==harvest_object_id).\\\n",
    "                                    first()\n",
    "\n",
    "        parts = urlparse.urlparse(obj.source.url)\n",
    "\n",
    "        params = {\n",
    "            'SERVICE': 'CSW',\n",
    "            'VERSION': '2.0.2',\n",
    "            'REQUEST': 'GetRecordById',\n",
    "            'OUTPUTSCHEMA': 'http://www.isotc211.org/2005/gmd',\n",
    "            'OUTPUTFORMAT':'application/xml' ,\n",
    "            'ID': obj.guid\n",
    "        }\n",
    "\n",
    "        url = urlparse.urlunparse((\n",
    "            parts.scheme,\n",
    "            parts.netloc,\n",
    "            parts.path,\n",
    "            None,\n",
    "            urllib.urlencode(params),\n",
    "            None\n",
    "        ))\n",
    "\n",
    "        return url\n",
    "\n",
    "    def output_schema(self):\n",
    "        return 'gmd'\n",
    "\n",
    "    def gather_stage(self, harvest_job):\n",
    "        log = logging.getLogger(__name__ + '.CSW.gather')\n",
    "        log.debug('CswHarvester gather_stage for job: %r', harvest_job)\n",
    "        # Get source URL\n",
    "        url = harvest_job.source.url\n",
    "\n",
    "        self._set_source_config(harvest_job.source.config)\n",
    "\n",
    "        try:\n",
    "            self._setup_csw_client(url)\n",
    "        except Exception, e:\n",
    "            self._save_gather_error('Error contacting the CSW server: %s' % e, harvest_job)\n",
    "            return None\n",
    "\n",
    "        query = model.Session.query(HarvestObject.guid, HarvestObject.package_id).\\\n",
    "                                    filter(HarvestObject.current==True).\\\n",
    "                                    filter(HarvestObject.harvest_source_id==harvest_job.source.id)\n",
    "        guid_to_package_id = {}\n",
    "\n",
    "        for guid, package_id in query:\n",
    "            guid_to_package_id[guid] = package_id\n",
    "\n",
    "        guids_in_db = set(guid_to_package_id.keys())\n",
    "\n",
    "        # extract cql filter if any\n",
    "        cql = self.source_config.get('cql')\n",
    "\n",
    "        log.debug('Starting gathering for %s' % url)\n",
    "        guids_in_harvest = set()\n",
    "        try:\n",
    "            for identifier in self.csw.getidentifiers(page=10, outputschema=self.output_schema(), cql=cql):\n",
    "                try:\n",
    "                    log.info('Got identifier %s from the CSW', identifier)\n",
    "                    if identifier is None:\n",
    "                        log.error('CSW returned identifier %r, skipping...' % identifier)\n",
    "                        continue\n",
    "\n",
    "                    guids_in_harvest.add(identifier)\n",
    "                except Exception, e:\n",
    "                    self._save_gather_error('Error for the identifier %s [%r]' % (identifier,e), harvest_job)\n",
    "                    continue\n",
    "\n",
    "\n",
    "        except Exception, e:\n",
    "            log.error('Exception: %s' % text_traceback())\n",
    "            self._save_gather_error('Error gathering the identifiers from the CSW server [%s]' % str(e), harvest_job)\n",
    "            return None\n",
    "\n",
    "        new = guids_in_harvest - guids_in_db\n",
    "        delete = guids_in_db - guids_in_harvest\n",
    "        change = guids_in_db & guids_in_harvest\n",
    "\n",
    "        ids = []\n",
    "        for guid in new:\n",
    "            obj = HarvestObject(guid=guid, job=harvest_job,\n",
    "                                extras=[HOExtra(key='status', value='new')])\n",
    "            obj.save()\n",
    "            ids.append(obj.id)\n",
    "        for guid in change:\n",
    "            obj = HarvestObject(guid=guid, job=harvest_job,\n",
    "                                package_id=guid_to_package_id[guid],\n",
    "                                extras=[HOExtra(key='status', value='change')])\n",
    "            obj.save()\n",
    "            ids.append(obj.id)\n",
    "        for guid in delete:\n",
    "            obj = HarvestObject(guid=guid, job=harvest_job,\n",
    "                                package_id=guid_to_package_id[guid],\n",
    "                                extras=[HOExtra(key='status', value='delete')])\n",
    "            model.Session.query(HarvestObject).\\\n",
    "                  filter_by(guid=guid).\\\n",
    "                  update({'current': False}, False)\n",
    "            obj.save()\n",
    "            ids.append(obj.id)\n",
    "\n",
    "        if len(ids) == 0:\n",
    "            self._save_gather_error('No records received from the CSW server', harvest_job)\n",
    "            return None\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def fetch_stage(self,harvest_object):\n",
    "\n",
    "        # Check harvest object status\n",
    "        status = self._get_object_extra(harvest_object, 'status')\n",
    "\n",
    "        if status == 'delete':\n",
    "            # No need to fetch anything, just pass to the import stage\n",
    "            return True\n",
    "\n",
    "        log = logging.getLogger(__name__ + '.CSW.fetch')\n",
    "        log.debug('CswHarvester fetch_stage for object: %s', harvest_object.id)\n",
    "\n",
    "        url = harvest_object.source.url\n",
    "        try:\n",
    "            self._setup_csw_client(url)\n",
    "        except Exception, e:\n",
    "            self._save_object_error('Error contacting the CSW server: %s' % e,\n",
    "                                    harvest_object)\n",
    "            return False\n",
    "\n",
    "        identifier = harvest_object.guid\n",
    "        try:\n",
    "            record = self.csw.getrecordbyid([identifier], outputschema=self.output_schema())\n",
    "        except Exception, e:\n",
    "            self._save_object_error('Error getting the CSW record with GUID %s' % identifier, harvest_object)\n",
    "            return False\n",
    "\n",
    "        if record is None:\n",
    "            self._save_object_error('Empty record for GUID %s' % identifier,\n",
    "                                    harvest_object)\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # Save the fetch contents in the HarvestObject\n",
    "            # Contents come from csw_client already declared and encoded as utf-8\n",
    "            # Remove original XML declaration\n",
    "            content = re.sub('<\\?xml(.*)\\?>', '', record['xml'])\n",
    "\n",
    "            harvest_object.content = content.strip()\n",
    "            harvest_object.save()\n",
    "        except Exception,e:\n",
    "            self._save_object_error('Error saving the harvest object for GUID %s [%r]' % \\\n",
    "                                    (identifier, e), harvest_object)\n",
    "            return False\n",
    "\n",
    "        log.debug('XML content saved (len %s)', len(record['xml']))\n",
    "        return True\n",
    "\n",
    "    def _setup_csw_client(self, url):\n",
    "        self.csw = CswService(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-ckan-ipykernel_py2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
